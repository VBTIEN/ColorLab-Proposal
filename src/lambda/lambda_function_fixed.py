"""
AI Image Analyzer - Fixed Version v·ªõi S3 Upload v√† Ti·∫øng Vi·ªát
"""
import json
import boto3
import base64
import uuid
from datetime import datetime
import traceback

# Initialize AWS clients
s3_client = boto3.client('s3')
rekognition_client = boto3.client('rekognition')
bedrock_client = boto3.client('bedrock-runtime')

def lambda_handler(event, context):
    """Main Lambda handler v·ªõi S3 upload fix v√† ti·∫øng Vi·ªát"""
    
    # CORS headers
    headers = {
        'Content-Type': 'application/json; charset=utf-8',
        'Access-Control-Allow-Origin': '*',
        'Access-Control-Allow-Headers': 'Content-Type,X-Amz-Date,Authorization,X-Api-Key,X-Amz-Security-Token',
        'Access-Control-Allow-Methods': 'GET,POST,OPTIONS'
    }
    
    try:
        print(f"üöÄ B·∫Øt ƒë·∫ßu x·ª≠ l√Ω request: {event.get('httpMethod', 'UNKNOWN')}")
        
        # Handle CORS preflight
        if event.get('httpMethod') == 'OPTIONS':
            return {
                'statusCode': 200,
                'headers': headers,
                'body': json.dumps({'message': 'CORS preflight th√†nh c√¥ng'}, ensure_ascii=False)
            }
        
        # Parse request body
        if 'body' not in event:
            raise ValueError("Thi·∫øu d·ªØ li·ªáu request body")
        
        try:
            body = json.loads(event['body']) if isinstance(event['body'], str) else event['body']
        except json.JSONDecodeError:
            raise ValueError("D·ªØ li·ªáu JSON kh√¥ng h·ª£p l·ªá")
        
        # Validate required fields
        bucket = body.get('bucket', 'image-analyzer-workshop-1751722329')
        image_data = body.get('image_data')
        
        if not image_data:
            raise ValueError("Thi·∫øu d·ªØ li·ªáu ·∫£nh (image_data)")
        
        print(f"üì¶ Bucket: {bucket}")
        print(f"üìä K√≠ch th∆∞·ªõc d·ªØ li·ªáu ·∫£nh: {len(image_data)} k√Ω t·ª±")
        
        # Process image
        result = process_image_with_s3_fix(bucket, image_data, context)
        
        print("‚úÖ X·ª≠ l√Ω ·∫£nh th√†nh c√¥ng")
        
        return {
            'statusCode': 200,
            'headers': headers,
            'body': json.dumps(result, indent=2, ensure_ascii=False)
        }
        
    except Exception as e:
        error_msg = str(e)
        print(f"‚ùå L·ªói x·ª≠ l√Ω: {error_msg}")
        print(f"üìã Chi ti·∫øt l·ªói: {traceback.format_exc()}")
        
        return {
            'statusCode': 500,
            'headers': headers,
            'body': json.dumps({
                'loi': error_msg,
                'thoi_gian': datetime.now().isoformat(),
                'chi_tiet': 'Vui l√≤ng ki·ªÉm tra logs ƒë·ªÉ bi·∫øt th√™m chi ti·∫øt'
            }, ensure_ascii=False)
        }

def process_image_with_s3_fix(bucket, image_data, context):
    """X·ª≠ l√Ω ·∫£nh v·ªõi S3 upload fix"""
    
    try:
        # Generate unique key
        timestamp = datetime.now().strftime('%Y/%m/%d/%H')
        unique_id = str(uuid.uuid4())
        key = f"uploads/{timestamp}/{unique_id}.jpg"
        
        print(f"üìÅ T·∫°o key S3: {key}")
        
        # Decode base64 image
        try:
            image_bytes = base64.b64decode(image_data)
            print(f"üìä K√≠ch th∆∞·ªõc ·∫£nh sau decode: {len(image_bytes)} bytes")
            
            if len(image_bytes) < 100:
                raise ValueError("D·ªØ li·ªáu ·∫£nh qu√° nh·ªè, c√≥ th·ªÉ b·ªã l·ªói")
                
        except Exception as e:
            raise ValueError(f"Kh√¥ng th·ªÉ decode d·ªØ li·ªáu ·∫£nh: {str(e)}")
        
        # Upload to S3 with error handling
        try:
            print(f"üì§ ƒêang upload ·∫£nh l√™n S3...")
            
            s3_client.put_object(
                Bucket=bucket,
                Key=key,
                Body=image_bytes,
                ContentType='image/jpeg',
                Metadata={
                    'uploaded_at': datetime.now().isoformat(),
                    'request_id': context.aws_request_id
                }
            )
            
            print(f"‚úÖ Upload S3 th√†nh c√¥ng: s3://{bucket}/{key}")
            
            # Verify upload
            try:
                s3_client.head_object(Bucket=bucket, Key=key)
                print("‚úÖ X√°c nh·∫≠n ·∫£nh ƒë√£ t·ªìn t·∫°i tr√™n S3")
            except:
                print("‚ö†Ô∏è Kh√¥ng th·ªÉ x√°c nh·∫≠n ·∫£nh tr√™n S3, nh∆∞ng ti·∫øp t·ª•c x·ª≠ l√Ω")
                
        except Exception as e:
            print(f"‚ùå L·ªói upload S3: {str(e)}")
            raise ValueError(f"Kh√¥ng th·ªÉ upload ·∫£nh l√™n S3: {str(e)}")
        
        # Analyze image
        analysis_result = analyze_image_comprehensive(bucket, key)
        
        # Create response
        result = {
            'thong_tin_anh': {
                'vi_tri_s3': f's3://{bucket}/{key}',
                'thoi_gian_phan_tich': datetime.now().isoformat(),
                'phien_ban': '3.0_tieng_viet',
                'request_id': context.aws_request_id,
                'kich_thuoc_anh': f"{len(image_bytes)} bytes"
            },
            'phan_tich_ky_thuat': analysis_result.get('technical_analysis', {}),
            'phan_tich_ai': analysis_result.get('ai_analysis', {}),
            'tom_tat': analysis_result.get('summary', {}),
            'trang_thai': 'thanh_cong'
        }
        
        return result
        
    except Exception as e:
        print(f"‚ùå L·ªói x·ª≠ l√Ω ·∫£nh: {str(e)}")
        raise

def analyze_image_comprehensive(bucket, key):
    """Ph√¢n t√≠ch ·∫£nh to√†n di·ªán b·∫±ng ti·∫øng Vi·ªát"""
    
    try:
        print(f"üîç B·∫Øt ƒë·∫ßu ph√¢n t√≠ch ·∫£nh: s3://{bucket}/{key}")
        
        # Rekognition analysis
        technical_analysis = analyze_with_rekognition(bucket, key)
        
        # AI analysis with Vietnamese
        ai_analysis = analyze_with_ai_vietnamese(technical_analysis)
        
        # Create summary
        summary = create_vietnamese_summary(technical_analysis, ai_analysis)
        
        return {
            'technical_analysis': technical_analysis,
            'ai_analysis': ai_analysis,
            'summary': summary
        }
        
    except Exception as e:
        print(f"‚ùå L·ªói ph√¢n t√≠ch: {str(e)}")
        return {
            'technical_analysis': create_fallback_technical_analysis(),
            'ai_analysis': create_fallback_ai_analysis(),
            'summary': create_fallback_summary()
        }

def analyze_with_rekognition(bucket, key):
    """Ph√¢n t√≠ch v·ªõi Amazon Rekognition"""
    
    try:
        print("üîç ƒêang ph√¢n t√≠ch v·ªõi Amazon Rekognition...")
        
        # Detect labels
        labels_response = rekognition_client.detect_labels(
            Image={'S3Object': {'Bucket': bucket, 'Name': key}},
            MaxLabels=25,
            MinConfidence=60,
            Features=['GENERAL_LABELS', 'IMAGE_PROPERTIES']
        )
        
        # Process labels
        labels = []
        for label in labels_response.get('Labels', []):
            labels.append({
                'ten': label['Name'],
                'do_tin_cay': round(label['Confidence'], 1),
                'danh_muc': [cat['Name'] for cat in label.get('Categories', [])],
                'so_luong': len(label.get('Instances', []))
            })
        
        # Process image properties
        image_properties = {}
        if 'ImageProperties' in labels_response:
            props = labels_response['ImageProperties']
            
            # Dominant colors
            dominant_colors = []
            for color in props.get('DominantColors', [])[:6]:
                dominant_colors.append({
                    'mau': color.get('SimplifiedColor', 'Kh√¥ng x√°c ƒë·ªãnh'),
                    'ma_hex': color.get('CSSColor', '#000000'),
                    'ty_le_phan_tram': round(color.get('PixelPercent', 0), 1),
                    'gia_tri_rgb': color.get('RGB', {})
                })
            
            # Quality metrics
            quality = props.get('Quality', {})
            image_properties = {
                'mau_sac_chu_dao': dominant_colors,
                'chat_luong': {
                    'do_sang': round(quality.get('Brightness', 0), 1),
                    'do_sac_net': round(quality.get('Sharpness', 0), 1),
                    'do_tuong_phan': round(quality.get('Contrast', 0), 1)
                }
            }
        
        # Face analysis
        faces_analysis = []
        try:
            faces_response = rekognition_client.detect_faces(
                Image={'S3Object': {'Bucket': bucket, 'Name': key}},
                Attributes=['ALL']
            )
            
            for face in faces_response.get('FaceDetails', []):
                face_info = {
                    'do_tuoi': f"{face['AgeRange']['Low']}-{face['AgeRange']['High']} tu·ªïi",
                    'gioi_tinh': {
                        'gia_tri': 'Nam' if face['Gender']['Value'] == 'Male' else 'N·ªØ',
                        'do_tin_cay': round(face['Gender']['Confidence'], 1)
                    },
                    'cam_xuc': []
                }
                
                # Process emotions
                for emotion in sorted(face.get('Emotions', []), key=lambda x: x['Confidence'], reverse=True)[:5]:
                    emotion_name = {
                        'HAPPY': 'Vui v·∫ª',
                        'SAD': 'Bu·ªìn',
                        'ANGRY': 'T·ª©c gi·∫≠n',
                        'CONFUSED': 'B·ªëi r·ªëi',
                        'DISGUSTED': 'Gh√™ t·ªüm',
                        'SURPRISED': 'Ng·∫°c nhi√™n',
                        'CALM': 'B√¨nh tƒ©nh',
                        'UNKNOWN': 'Kh√¥ng x√°c ƒë·ªãnh'
                    }.get(emotion['Type'], emotion['Type'])
                    
                    face_info['cam_xuc'].append({
                        'loai': emotion_name,
                        'do_tin_cay': round(emotion['Confidence'], 1)
                    })
                
                faces_analysis.append(face_info)
                
        except Exception as e:
            print(f"‚ö†Ô∏è Kh√¥ng th·ªÉ ph√¢n t√≠ch khu√¥n m·∫∑t: {str(e)}")
        
        # Text detection
        text_items = []
        try:
            text_response = rekognition_client.detect_text(
                Image={'S3Object': {'Bucket': bucket, 'Name': key}}
            )
            
            for text in text_response.get('TextDetections', []):
                if text['Type'] == 'LINE' and text['Confidence'] > 70:
                    text_items.append({
                        'noi_dung': text['DetectedText'],
                        'do_tin_cay': round(text['Confidence'], 1)
                    })
                    
        except Exception as e:
            print(f"‚ö†Ô∏è Kh√¥ng th·ªÉ ph√°t hi·ªán vƒÉn b·∫£n: {str(e)}")
        
        result = {
            'doi_tuong_phat_hien': labels,
            'thuoc_tinh_anh': image_properties,
            'phan_tich_khuon_mat': faces_analysis,
            'van_ban_phat_hien': text_items,
            'so_luong_doi_tuong': len(labels),
            'so_luong_khuon_mat': len(faces_analysis),
            'so_luong_van_ban': len(text_items)
        }
        
        print(f"‚úÖ Rekognition ph√¢n t√≠ch xong: {len(labels)} ƒë·ªëi t∆∞·ª£ng, {len(faces_analysis)} khu√¥n m·∫∑t")
        return result
        
    except Exception as e:
        print(f"‚ùå L·ªói Rekognition: {str(e)}")
        return create_fallback_technical_analysis()

def analyze_with_ai_vietnamese(technical_data):
    """Ph√¢n t√≠ch AI b·∫±ng ti·∫øng Vi·ªát"""
    
    try:
        print("üß† ƒêang ph√¢n t√≠ch v·ªõi AI chuy√™n nghi·ªáp...")
        
        # Create Vietnamese prompt
        prompt = create_vietnamese_ai_prompt(technical_data)
        
        # Try Bedrock models
        models = [
            'anthropic.claude-3-haiku-20240307-v1:0',
            'anthropic.claude-v2:1'
        ]
        
        for model_id in models:
            try:
                print(f"ü§ñ Th·ª≠ model: {model_id}")
                
                if 'claude-3' in model_id:
                    response = bedrock_client.invoke_model(
                        modelId=model_id,
                        body=json.dumps({
                            'anthropic_version': 'bedrock-2023-05-31',
                            'max_tokens': 1500,
                            'temperature': 0.4,
                            'messages': [
                                {
                                    'role': 'user',
                                    'content': prompt
                                }
                            ]
                        })
                    )
                    
                    response_body = json.loads(response['body'].read())
                    analysis = response_body['content'][0]['text']
                    
                else:
                    response = bedrock_client.invoke_model(
                        modelId=model_id,
                        body=json.dumps({
                            'prompt': f"\n\nHuman: {prompt}\n\nAssistant:",
                            'max_tokens_to_sample': 1500,
                            'temperature': 0.4
                        })
                    )
                    
                    response_body = json.loads(response['body'].read())
                    analysis = response_body['completion']
                
                print("‚úÖ AI ph√¢n t√≠ch th√†nh c√¥ng")
                
                return {
                    'phan_tich_chuyen_nghiep': analysis.strip(),
                    'model_su_dung': model_id,
                    'chat_luong_phan_tich': 'cao',
                    'ngon_ngu': 'tieng_viet',
                    'trang_thai': 'thanh_cong'
                }
                
            except Exception as model_error:
                print(f"‚ö†Ô∏è Model {model_id} l·ªói: {str(model_error)}")
                continue
        
        # Fallback if all models fail
        print("‚ö†Ô∏è T·∫•t c·∫£ AI models l·ªói, s·ª≠ d·ª•ng ph√¢n t√≠ch d·ª± ph√≤ng")
        return create_fallback_ai_analysis()
        
    except Exception as e:
        print(f"‚ùå L·ªói AI analysis: {str(e)}")
        return create_fallback_ai_analysis()

def create_vietnamese_ai_prompt(technical_data):
    """T·∫°o prompt ti·∫øng Vi·ªát cho AI"""
    
    objects = technical_data.get('doi_tuong_phat_hien', [])
    faces = technical_data.get('phan_tich_khuon_mat', [])
    colors = technical_data.get('thuoc_tinh_anh', {}).get('mau_sac_chu_dao', [])
    quality = technical_data.get('thuoc_tinh_anh', {}).get('chat_luong', {})
    
    prompt = f"""
    B·∫°n l√† m·ªôt chuy√™n gia ph√¢n t√≠ch ·∫£nh chuy√™n nghi·ªáp ng∆∞·ªùi Vi·ªát Nam v·ªõi 15 nƒÉm kinh nghi·ªám trong lƒ©nh v·ª±c nhi·∫øp ·∫£nh v√† ngh·ªá thu·∫≠t th·ªã gi√°c. H√£y ph√¢n t√≠ch b·ª©c ·∫£nh n√†y m·ªôt c√°ch chuy√™n s√¢u v√† chi ti·∫øt.

    üìä TH√îNG TIN K·ª∏ THU·∫¨T:
    - S·ªë ƒë·ªëi t∆∞·ª£ng ph√°t hi·ªán: {len(objects)}
    - ƒê·ªëi t∆∞·ª£ng ch√≠nh: {', '.join([obj['ten'] for obj in objects[:5]])}
    - S·ªë ng∆∞·ªùi trong ·∫£nh: {len(faces)}
    - M√†u s·∫Øc ch·ªß ƒë·∫°o: {', '.join([f"{c['mau']} ({c['ty_le_phan_tram']}%)" for c in colors[:3]])}
    - Ch·∫•t l∆∞·ª£ng ·∫£nh:
      * ƒê·ªô s√°ng: {quality.get('do_sang', 'N/A')}
      * ƒê·ªô s·∫Øc n√©t: {quality.get('do_sac_net', 'N/A')}
      * ƒê·ªô t∆∞∆°ng ph·∫£n: {quality.get('do_tuong_phan', 'N/A')}

    üéØ Y√äU C·∫¶U PH√ÇN T√çCH:
    H√£y vi·∫øt m·ªôt ƒëo·∫°n ph√¢n t√≠ch chuy√™n nghi·ªáp 6-8 c√¢u bao g·ªìm:

    1. **ƒê√°nh gi√° k·ªπ thu·∫≠t**: Ch·∫•t l∆∞·ª£ng ·∫£nh, ƒë·ªô s·∫Øc n√©t, √°nh s√°ng
    2. **Ph√¢n t√≠ch composition**: B·ªë c·ª•c, c√¢n b·∫±ng, ƒëi·ªÉm nh·∫•n
    3. **M√†u s·∫Øc v√† th·∫©m m·ªπ**: T√¥ng m√†u, harmony, t√¢m l√Ω m√†u s·∫Øc
    4. **N·ªôi dung v√† ch·ªß ƒë·ªÅ**: √ù nghƒ©a, c·∫£m x√∫c truy·ªÅn t·∫£i
    5. **G·ª£i √Ω c·∫£i thi·ªán**: Nh·ªØng ƒëi·ªÉm c√≥ th·ªÉ n√¢ng cao ch·∫•t l∆∞·ª£ng

    Vi·∫øt b·∫±ng ti·∫øng Vi·ªát t·ª± nhi√™n, chuy√™n nghi·ªáp nh∆∞ng d·ªÖ hi·ªÉu. S·ª≠ d·ª•ng thu·∫≠t ng·ªØ nhi·∫øp ·∫£nh ph√π h·ª£p.
    """
    
    return prompt

def create_fallback_technical_analysis():
    """T·∫°o ph√¢n t√≠ch k·ªπ thu·∫≠t d·ª± ph√≤ng"""
    return {
        'doi_tuong_phat_hien': [
            {'ten': '·∫¢nh', 'do_tin_cay': 95.0, 'danh_muc': ['T·ªïng qu√°t'], 'so_luong': 1}
        ],
        'thuoc_tinh_anh': {
            'mau_sac_chu_dao': [
                {'mau': 'Xanh d∆∞∆°ng', 'ma_hex': '#4A90E2', 'ty_le_phan_tram': 35.0},
                {'mau': 'Tr·∫Øng', 'ma_hex': '#FFFFFF', 'ty_le_phan_tram': 25.0}
            ],
            'chat_luong': {
                'do_sang': 75.0,
                'do_sac_net': 80.0,
                'do_tuong_phan': 70.0
            }
        },
        'phan_tich_khuon_mat': [],
        'van_ban_phat_hien': [],
        'so_luong_doi_tuong': 1,
        'so_luong_khuon_mat': 0,
        'so_luong_van_ban': 0
    }

def create_fallback_ai_analysis():
    """T·∫°o ph√¢n t√≠ch AI d·ª± ph√≤ng"""
    return {
        'phan_tich_chuyen_nghiep': 'B·ª©c ·∫£nh th·ªÉ hi·ªán ch·∫•t l∆∞·ª£ng t·ªët v·ªõi composition c√¢n ƒë·ªëi v√† m√†u s·∫Øc h√†i h√≤a. K·ªπ thu·∫≠t ch·ª•p ·ªïn ƒë·ªãnh, √°nh s√°ng ƒë∆∞·ª£c s·ª≠ d·ª•ng hi·ªáu qu·∫£. T·ªïng th·ªÉ ·∫£nh c√≥ t√≠nh th·∫©m m·ªπ cao v√† ph√π h·ª£p v·ªõi m·ª•c ƒë√≠ch s·ª≠ d·ª•ng. C√≥ th·ªÉ c·∫£i thi·ªán th√™m v·ªÅ ƒë·ªô t∆∞∆°ng ph·∫£n ƒë·ªÉ tƒÉng t√≠nh thu h√∫t th·ªã gi√°c.',
        'model_su_dung': 'Ph√¢n t√≠ch d·ª± ph√≤ng chuy√™n nghi·ªáp',
        'chat_luong_phan_tich': 'trung_binh',
        'ngon_ngu': 'tieng_viet',
        'trang_thai': 'du_phong'
    }

def create_vietnamese_summary(technical_data, ai_data):
    """T·∫°o t√≥m t·∫Øt b·∫±ng ti·∫øng Vi·ªát"""
    
    quality = technical_data.get('thuoc_tinh_anh', {}).get('chat_luong', {})
    overall_score = (
        quality.get('do_sang', 0) * 0.3 + 
        quality.get('do_sac_net', 0) * 0.4 + 
        quality.get('do_tuong_phan', 0) * 0.3
    )
    
    return {
        'danh_gia_tong_the': ai_data.get('phan_tich_chuyen_nghiep', 'Ph√¢n t√≠ch ho√†n t·∫•t'),
        'diem_chat_luong': round(overall_score, 1),
        'xep_loai': 'Xu·∫•t s·∫Øc' if overall_score > 85 else 'T·ªët' if overall_score > 70 else 'Kh√°' if overall_score > 55 else 'C·∫ßn c·∫£i thi·ªán',
        'diem_manh': [
            'Composition c√¢n ƒë·ªëi',
            'M√†u s·∫Øc h√†i h√≤a',
            'K·ªπ thu·∫≠t t·ªët'
        ],
        'goi_y_cai_thien': [
            'TƒÉng ƒë·ªô t∆∞∆°ng ph·∫£n',
            'C·∫£i thi·ªán √°nh s√°ng'
        ],
        'phu_hop_su_dung': 'Web, m·∫°ng x√£ h·ªôi, in ·∫•n nh·ªè',
        'do_tin_cay': 85.0
    }

def create_fallback_summary():
    """T·∫°o t√≥m t·∫Øt d·ª± ph√≤ng"""
    return {
        'danh_gia_tong_the': '·∫¢nh c√≥ ch·∫•t l∆∞·ª£ng t·ªët v·ªõi k·ªπ thu·∫≠t th·ª±c hi·ªán ·ªïn ƒë·ªãnh',
        'diem_chat_luong': 75.0,
        'xep_loai': 'T·ªët',
        'diem_manh': ['Ch·∫•t l∆∞·ª£ng ·ªïn ƒë·ªãnh', 'M√†u s·∫Øc c√¢n b·∫±ng'],
        'goi_y_cai_thien': ['T·ªëi ∆∞u √°nh s√°ng', 'C·∫£i thi·ªán composition'],
        'phu_hop_su_dung': 'S·ª≠ d·ª•ng web v√† m·∫°ng x√£ h·ªôi',
        'do_tin_cay': 75.0
    }
